{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input,LSTM,Dense\n",
    "from keras.models import Model,load_model\n",
    "from keras.utils import plot_model\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_input,n_output,n_units):\n",
    "    #训练阶段\n",
    "    #encoder\n",
    "    encoder_input = Input(shape = (None, n_input))\n",
    "    #encoder输入维度n_input为每个时间步的输入xt的维度，这里是用来one-hot的英文字符数\n",
    "    encoder = LSTM(n_units, return_state=True)\n",
    "    #n_units为LSTM单元中每个门的神经元的个数，return_state设为True时才会返回最后时刻的状态h,c\n",
    "    _,encoder_h,encoder_c = encoder(encoder_input)\n",
    "    encoder_state = [encoder_h,encoder_c]\n",
    "    #保留下来encoder的末状态作为decoder的初始状态\n",
    "    \n",
    "    #decoder\n",
    "    decoder_input = Input(shape = (None, n_output))\n",
    "    #decoder的输入维度为中文字符数\n",
    "    decoder = LSTM(n_units,return_sequences=True, return_state=True)\n",
    "    #训练模型时需要decoder的输出序列来与结果对比优化，故return_sequences也要设为True\n",
    "    decoder_output, _, _ = decoder(decoder_input,initial_state=encoder_state)\n",
    "    #在训练阶段只需要用到decoder的输出序列，不需要用最终状态h.c\n",
    "    decoder_dense = Dense(n_output,activation='softmax')\n",
    "    decoder_output = decoder_dense(decoder_output)\n",
    "    #输出序列经过全连接层得到结果\n",
    "    \n",
    "    #生成的训练模型\n",
    "    model = Model([encoder_input,decoder_input],decoder_output)\n",
    "    #第一个参数为训练模型的输入，包含了encoder和decoder的输入，第二个参数为模型的输出，包含了decoder的输出\n",
    "    \n",
    "    #推理阶段，用于预测过程\n",
    "    #推断模型—encoder\n",
    "    encoder_infer = Model(encoder_input,encoder_state)\n",
    "    \n",
    "    #推断模型-decoder\n",
    "    decoder_state_input_h = Input(shape=(n_units,))\n",
    "    decoder_state_input_c = Input(shape=(n_units,))    \n",
    "    decoder_state_input = [decoder_state_input_h, decoder_state_input_c]#上个时刻的状态h,c   \n",
    "    \n",
    "    decoder_infer_output, decoder_infer_state_h, decoder_infer_state_c = decoder(decoder_input,initial_state=decoder_state_input)\n",
    "    decoder_infer_state = [decoder_infer_state_h, decoder_infer_state_c]#当前时刻得到的状态\n",
    "    decoder_infer_output = decoder_dense(decoder_infer_output)#当前时刻的输出\n",
    "    decoder_infer = Model([decoder_input]+decoder_state_input,[decoder_infer_output]+decoder_infer_state)\n",
    "    \n",
    "    return model, encoder_infer, decoder_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_UNITS = 256\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 200\n",
    "NUM_SAMPLES = 10000\n",
    "data_path = 'cmn.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(data_path,header=None).iloc[:NUM_SAMPLES,:,]\n",
    "df.columns=['inputs','targets']\n",
    "\n",
    "df['targets'] = df['targets'].apply(lambda x: '\\t'+x+'\\n')\n",
    "\n",
    "input_texts = df.inputs.values.tolist()\n",
    "target_texts = df.targets.values.tolist()\n",
    "\n",
    "input_characters = sorted(list(set(df.inputs.unique().sum())))\n",
    "target_characters = sorted(list(set(df.targets.unique().sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INUPT_LENGTH = max([len(i) for i in input_texts])\n",
    "OUTPUT_LENGTH = max([len(i) for i in target_texts])\n",
    "INPUT_FEATURE_LENGTH = len(input_characters)\n",
    "OUTPUT_FEATURE_LENGTH = len(target_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = np.zeros((NUM_SAMPLES,INUPT_LENGTH,INPUT_FEATURE_LENGTH))\n",
    "decoder_input = np.zeros((NUM_SAMPLES,OUTPUT_LENGTH,OUTPUT_FEATURE_LENGTH))\n",
    "decoder_output = np.zeros((NUM_SAMPLES,OUTPUT_LENGTH,OUTPUT_FEATURE_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = {char:index for index,char in enumerate(input_characters)}\n",
    "input_dict_reverse = {index:char for index,char in enumerate(input_characters)}\n",
    "target_dict = {char:index for index,char in enumerate(target_characters)}\n",
    "target_dict_reverse = {index:char for index,char in enumerate(target_characters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index,seq in enumerate(input_texts):\n",
    "    for char_index, char in enumerate(seq):\n",
    "        encoder_input[seq_index,char_index,input_dict[char]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index,seq in enumerate(target_texts):\n",
    "    for char_index,char in enumerate(seq):\n",
    "        decoder_input[seq_index,char_index,target_dict[char]] = 1.0\n",
    "        if char_index > 0:\n",
    "            decoder_output[seq_index,char_index-1,target_dict[char]] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 观察向量化的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([input_dict_reverse[np.argmax(i)] for i in encoder_input[0] if max(i) !=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'嗨。\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([target_dict_reverse[np.argmax(i)] for i in decoder_output[0] if max(i) !=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t嗨。\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([target_dict_reverse[np.argmax(i)] for i in decoder_input[0] if max(i) !=0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model_train, encoder_infer, decoder_infer = create_model(INPUT_FEATURE_LENGTH, OUTPUT_FEATURE_LENGTH, N_UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看模型结构\n",
    "plot_model(to_file='model.png',model=model_train,show_shapes=True)\n",
    "plot_model(to_file='encoder.png',model=encoder_infer,show_shapes=True)\n",
    "plot_model(to_file='decoder.png',model=decoder_infer,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 73)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 2623)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 337920      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  2949120     input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2623)   674111      lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,961,151\n",
      "Trainable params: 3,961,151\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, 73)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                [(None, 256), (None, 256) 337920    \n",
      "=================================================================\n",
      "Total params: 337,920\n",
      "Trainable params: 337,920\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_infer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None, 2623)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  2949120     input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2623)   674111      lstm_2[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,623,231\n",
      "Trainable params: 3,623,231\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_infer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/200\n",
      "8000/8000 [==============================] - 89s 11ms/step - loss: 2.0337 - val_loss: 2.5737\n",
      "Epoch 2/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 1.9065 - val_loss: 2.4193\n",
      "Epoch 3/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 1.8117 - val_loss: 2.3393\n",
      "Epoch 4/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 1.7218 - val_loss: 2.2300\n",
      "Epoch 5/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 1.6390 - val_loss: 2.1807\n",
      "Epoch 6/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 1.5729 - val_loss: 2.1130\n",
      "Epoch 7/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 1.5037 - val_loss: 2.0633\n",
      "Epoch 8/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 1.4464 - val_loss: 1.9959\n",
      "Epoch 9/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 1.3920 - val_loss: 1.9566\n",
      "Epoch 10/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 1.3423 - val_loss: 1.9343\n",
      "Epoch 11/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 1.2985 - val_loss: 1.8930\n",
      "Epoch 12/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 1.2577 - val_loss: 1.8768\n",
      "Epoch 13/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 1.2203 - val_loss: 1.8623\n",
      "Epoch 14/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 1.1851 - val_loss: 1.8475\n",
      "Epoch 15/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 1.1533 - val_loss: 1.8335\n",
      "Epoch 16/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 1.1217 - val_loss: 1.8215\n",
      "Epoch 17/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 1.0922 - val_loss: 1.8129\n",
      "Epoch 18/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 1.0649 - val_loss: 1.8067\n",
      "Epoch 19/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 1.0376 - val_loss: 1.7971\n",
      "Epoch 20/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 1.0119 - val_loss: 1.7934\n",
      "Epoch 21/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.9884 - val_loss: 1.7990\n",
      "Epoch 22/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.9627 - val_loss: 1.7858\n",
      "Epoch 23/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.9399 - val_loss: 1.7827\n",
      "Epoch 24/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.9167 - val_loss: 1.7835\n",
      "Epoch 25/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.8938 - val_loss: 1.7852\n",
      "Epoch 26/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.8720 - val_loss: 1.7876\n",
      "Epoch 27/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.8504 - val_loss: 1.7913\n",
      "Epoch 28/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.8296 - val_loss: 1.7954\n",
      "Epoch 29/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.8095 - val_loss: 1.7950\n",
      "Epoch 30/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.7897 - val_loss: 1.7988\n",
      "Epoch 31/200\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.7708 - val_loss: 1.8010\n",
      "Epoch 32/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.7513 - val_loss: 1.8094\n",
      "Epoch 33/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.7335 - val_loss: 1.8092\n",
      "Epoch 34/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.7154 - val_loss: 1.8160\n",
      "Epoch 35/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.6977 - val_loss: 1.8231\n",
      "Epoch 36/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.6803 - val_loss: 1.8199\n",
      "Epoch 37/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.6631 - val_loss: 1.8196\n",
      "Epoch 38/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.6470 - val_loss: 1.8375\n",
      "Epoch 39/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.6315 - val_loss: 1.8439\n",
      "Epoch 40/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.6164 - val_loss: 1.8469\n",
      "Epoch 41/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.6007 - val_loss: 1.8508\n",
      "Epoch 42/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.5854 - val_loss: 1.8561\n",
      "Epoch 43/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.5716 - val_loss: 1.8662\n",
      "Epoch 44/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.5573 - val_loss: 1.8726\n",
      "Epoch 45/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.5440 - val_loss: 1.8781\n",
      "Epoch 46/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.5293 - val_loss: 1.8811\n",
      "Epoch 47/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.5171 - val_loss: 1.8868\n",
      "Epoch 48/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.5038 - val_loss: 1.9056\n",
      "Epoch 49/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.4916 - val_loss: 1.9146\n",
      "Epoch 50/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.4790 - val_loss: 1.9178\n",
      "Epoch 51/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.4679 - val_loss: 1.9220\n",
      "Epoch 52/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.4556 - val_loss: 1.9271\n",
      "Epoch 53/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.4455 - val_loss: 1.9442\n",
      "Epoch 54/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.4341 - val_loss: 1.9370\n",
      "Epoch 55/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.4237 - val_loss: 1.9624\n",
      "Epoch 56/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.4132 - val_loss: 1.9651\n",
      "Epoch 57/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.4021 - val_loss: 1.9776\n",
      "Epoch 58/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.3921 - val_loss: 1.9812\n",
      "Epoch 59/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.3822 - val_loss: 1.9931\n",
      "Epoch 60/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.3732 - val_loss: 1.9923\n",
      "Epoch 61/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.3633 - val_loss: 2.0009\n",
      "Epoch 62/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.3542 - val_loss: 2.0059\n",
      "Epoch 63/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.3454 - val_loss: 2.0161\n",
      "Epoch 64/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.3370 - val_loss: 2.0213\n",
      "Epoch 65/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.3285 - val_loss: 2.0290\n",
      "Epoch 66/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.3196 - val_loss: 2.0438\n",
      "Epoch 67/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.3116 - val_loss: 2.0416\n",
      "Epoch 68/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.3042 - val_loss: 2.0472\n",
      "Epoch 69/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.2969 - val_loss: 2.0562\n",
      "Epoch 70/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.2879 - val_loss: 2.0771\n",
      "Epoch 71/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.2805 - val_loss: 2.0753\n",
      "Epoch 72/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.2746 - val_loss: 2.0841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.2660 - val_loss: 2.0927\n",
      "Epoch 74/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.2591 - val_loss: 2.1024\n",
      "Epoch 75/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.2521 - val_loss: 2.1094\n",
      "Epoch 76/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.2450 - val_loss: 2.1230\n",
      "Epoch 77/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.2389 - val_loss: 2.1351\n",
      "Epoch 78/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.2340 - val_loss: 2.1321\n",
      "Epoch 79/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.2263 - val_loss: 2.1432\n",
      "Epoch 80/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.2193 - val_loss: 2.1494\n",
      "Epoch 81/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.2129 - val_loss: 2.1461\n",
      "Epoch 82/200\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.2071 - val_loss: 2.1667\n",
      "Epoch 83/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.2017 - val_loss: 2.1757\n",
      "Epoch 84/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.1950 - val_loss: 2.1793\n",
      "Epoch 85/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.1892 - val_loss: 2.2041\n",
      "Epoch 86/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.1835 - val_loss: 2.1972\n",
      "Epoch 87/200\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.1781 - val_loss: 2.2055\n",
      "Epoch 88/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.1717 - val_loss: 2.2239\n",
      "Epoch 89/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.1667 - val_loss: 2.2269\n",
      "Epoch 90/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.1621 - val_loss: 2.2319\n",
      "Epoch 91/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.1578 - val_loss: 2.2526\n",
      "Epoch 92/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.1523 - val_loss: 2.2564\n",
      "Epoch 93/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.1479 - val_loss: 2.2562\n",
      "Epoch 94/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.1424 - val_loss: 2.2573\n",
      "Epoch 95/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.1380 - val_loss: 2.2776\n",
      "Epoch 96/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.1334 - val_loss: 2.2795\n",
      "Epoch 97/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.1292 - val_loss: 2.2850\n",
      "Epoch 98/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.1258 - val_loss: 2.2914\n",
      "Epoch 99/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.1215 - val_loss: 2.3092\n",
      "Epoch 100/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.1166 - val_loss: 2.3048\n",
      "Epoch 101/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.1134 - val_loss: 2.3258\n",
      "Epoch 102/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.1086 - val_loss: 2.3211\n",
      "Epoch 103/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.1055 - val_loss: 2.3389\n",
      "Epoch 104/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.1018 - val_loss: 2.3457\n",
      "Epoch 105/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0984 - val_loss: 2.3558\n",
      "Epoch 106/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0944 - val_loss: 2.3629\n",
      "Epoch 107/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0912 - val_loss: 2.3671\n",
      "Epoch 108/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0881 - val_loss: 2.3685\n",
      "Epoch 109/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0849 - val_loss: 2.3831\n",
      "Epoch 110/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0818 - val_loss: 2.3880\n",
      "Epoch 111/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0792 - val_loss: 2.3983\n",
      "Epoch 112/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0765 - val_loss: 2.3932\n",
      "Epoch 113/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0731 - val_loss: 2.4024\n",
      "Epoch 114/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0708 - val_loss: 2.4131\n",
      "Epoch 115/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0684 - val_loss: 2.4225\n",
      "Epoch 116/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0663 - val_loss: 2.4265\n",
      "Epoch 117/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0634 - val_loss: 2.4337\n",
      "Epoch 118/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0611 - val_loss: 2.4444\n",
      "Epoch 119/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0592 - val_loss: 2.4454\n",
      "Epoch 120/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0569 - val_loss: 2.4498\n",
      "Epoch 121/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0550 - val_loss: 2.4626\n",
      "Epoch 122/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0529 - val_loss: 2.4663\n",
      "Epoch 123/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0514 - val_loss: 2.4711\n",
      "Epoch 124/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0493 - val_loss: 2.4779\n",
      "Epoch 125/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0481 - val_loss: 2.4810\n",
      "Epoch 126/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0455 - val_loss: 2.4863\n",
      "Epoch 127/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0449 - val_loss: 2.5031\n",
      "Epoch 128/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0434 - val_loss: 2.4960\n",
      "Epoch 129/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0419 - val_loss: 2.5094\n",
      "Epoch 130/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0399 - val_loss: 2.5149\n",
      "Epoch 131/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0389 - val_loss: 2.5165\n",
      "Epoch 132/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0378 - val_loss: 2.5327\n",
      "Epoch 133/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0364 - val_loss: 2.5299\n",
      "Epoch 134/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0357 - val_loss: 2.5304\n",
      "Epoch 135/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0345 - val_loss: 2.5360\n",
      "Epoch 136/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0337 - val_loss: 2.5332\n",
      "Epoch 137/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0328 - val_loss: 2.5542\n",
      "Epoch 138/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0307 - val_loss: 2.5496\n",
      "Epoch 139/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0309 - val_loss: 2.5572\n",
      "Epoch 140/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0296 - val_loss: 2.5634\n",
      "Epoch 141/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0288 - val_loss: 2.5745\n",
      "Epoch 142/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0281 - val_loss: 2.5689\n",
      "Epoch 143/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0274 - val_loss: 2.5852\n",
      "Epoch 144/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0267 - val_loss: 2.5810\n",
      "Epoch 145/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0256 - val_loss: 2.5927\n",
      "Epoch 146/200\n",
      "8000/8000 [==============================] - 82s 10ms/step - loss: 0.0254 - val_loss: 2.5903\n",
      "Epoch 147/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0250 - val_loss: 2.5971\n",
      "Epoch 148/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0246 - val_loss: 2.6028\n",
      "Epoch 149/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0237 - val_loss: 2.6092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0248 - val_loss: 2.5981\n",
      "Epoch 151/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0227 - val_loss: 2.6024\n",
      "Epoch 152/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0223 - val_loss: 2.6232\n",
      "Epoch 153/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0224 - val_loss: 2.6127\n",
      "Epoch 154/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0212 - val_loss: 2.6205\n",
      "Epoch 155/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0211 - val_loss: 2.6247\n",
      "Epoch 156/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0215 - val_loss: 2.6288\n",
      "Epoch 157/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0203 - val_loss: 2.6319\n",
      "Epoch 158/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0198 - val_loss: 2.6311\n",
      "Epoch 159/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0197 - val_loss: 2.6342\n",
      "Epoch 160/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0187 - val_loss: 2.6412\n",
      "Epoch 161/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0189 - val_loss: 2.6487\n",
      "Epoch 162/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0186 - val_loss: 2.6435\n",
      "Epoch 163/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0180 - val_loss: 2.6625\n",
      "Epoch 164/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0177 - val_loss: 2.6532\n",
      "Epoch 165/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0198 - val_loss: 2.6539\n",
      "Epoch 166/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0173 - val_loss: 2.6666\n",
      "Epoch 167/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0171 - val_loss: 2.6592\n",
      "Epoch 168/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0169 - val_loss: 2.6710\n",
      "Epoch 169/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0163 - val_loss: 2.6837\n",
      "Epoch 170/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0161 - val_loss: 2.6799\n",
      "Epoch 171/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0165 - val_loss: 2.6916\n",
      "Epoch 172/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0156 - val_loss: 2.6852\n",
      "Epoch 173/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0153 - val_loss: 2.6972\n",
      "Epoch 174/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0156 - val_loss: 2.6917\n",
      "Epoch 175/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0150 - val_loss: 2.7050\n",
      "Epoch 176/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0149 - val_loss: 2.6971\n",
      "Epoch 177/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0151 - val_loss: 2.6907\n",
      "Epoch 178/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0147 - val_loss: 2.7052\n",
      "Epoch 179/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0140 - val_loss: 2.7129\n",
      "Epoch 180/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0145 - val_loss: 2.7123\n",
      "Epoch 181/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0144 - val_loss: 2.7034\n",
      "Epoch 182/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0138 - val_loss: 2.7093\n",
      "Epoch 183/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0135 - val_loss: 2.7100\n",
      "Epoch 184/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0136 - val_loss: 2.7244\n",
      "Epoch 185/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0133 - val_loss: 2.7222\n",
      "Epoch 186/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0135 - val_loss: 2.7244\n",
      "Epoch 187/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0131 - val_loss: 2.7281\n",
      "Epoch 188/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0133 - val_loss: 2.7299\n",
      "Epoch 189/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0129 - val_loss: 2.7344\n",
      "Epoch 190/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0132 - val_loss: 2.7347\n",
      "Epoch 191/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0131 - val_loss: 2.7364\n",
      "Epoch 192/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0122 - val_loss: 2.7406\n",
      "Epoch 193/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0122 - val_loss: 2.7425\n",
      "Epoch 194/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0123 - val_loss: 2.7561\n",
      "Epoch 195/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0131 - val_loss: 2.7418\n",
      "Epoch 196/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0119 - val_loss: 2.7576\n",
      "Epoch 197/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0122 - val_loss: 2.7467\n",
      "Epoch 198/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0118 - val_loss: 2.7582\n",
      "Epoch 199/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0122 - val_loss: 2.7645\n",
      "Epoch 200/200\n",
      "8000/8000 [==============================] - 83s 10ms/step - loss: 0.0118 - val_loss: 2.7599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe0b0506b00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_train.fit([encoder_input,decoder_input],decoder_output,batch_size=BATCH_SIZE,epochs=EPOCH,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_chinese(source,encoder_inference, decoder_inference, n_steps, features):\n",
    "    #先通过推理encoder获得预测输入序列的隐状态\n",
    "    state = encoder_inference.predict(source)\n",
    "    #第一个字符'\\t',为起始标志\n",
    "    predict_seq = np.zeros((1,1,features))\n",
    "    predict_seq[0,0,target_dict['\\t']] = 1\n",
    "\n",
    "    output = ''\n",
    "    #开始对encoder获得的隐状态进行推理\n",
    "    #每次循环用上次预测的字符作为输入来预测下一次的字符，直到预测出了终止符\n",
    "    for i in range(n_steps):#n_steps为句子最大长度\n",
    "        #给decoder输入上一个时刻的h,c隐状态，以及上一次的预测字符predict_seq\n",
    "        yhat,h,c = decoder_inference.predict([predict_seq]+state)\n",
    "        #注意，这里的yhat为Dense之后输出的结果，因此与h不同\n",
    "        char_index = np.argmax(yhat[0,-1,:])\n",
    "        char = target_dict_reverse[char_index]\n",
    "        output += char\n",
    "        state = [h,c]#本次状态做为下一次的初始状态继续传递\n",
    "        predict_seq = np.zeros((1,1,features))\n",
    "        predict_seq[0,0,char_index] = 1\n",
    "        if char == '\\n':#预测到了终止符则停下来\n",
    "            break\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have brothers.\n",
      "我有兄弟。\n",
      "\n",
      "I have ten pens.\n",
      "我有十支筆。\n",
      "\n",
      "I have to hurry!\n",
      "我要赶紧了!\n",
      "\n",
      "I have two cats.\n",
      "我有两只猫。\n",
      "\n",
      "I have two sons.\n",
      "我有兩個兒子。\n",
      "\n",
      "I just threw up.\n",
      "我剛才吐了。\n",
      "\n",
      "I lent him a CD.\n",
      "我借给他一盘CD。\n",
      "\n",
      "I like Tom, too.\n",
      "我也喜歡湯姆。\n",
      "\n",
      "I like football.\n",
      "我喜歡足球。\n",
      "\n",
      "I like potatoes.\n",
      "我喜歡土豆。\n",
      "\n",
      "I like the cold.\n",
      "我喜歡寒冷。\n",
      "\n",
      "I like this dog.\n",
      "我喜欢这只狗。\n",
      "\n",
      "I like your car.\n",
      "我喜欢您的车。\n",
      "\n",
      "I lived in Rome.\n",
      "我住在羅馬。\n",
      "\n",
      "I love this car.\n",
      "我愛這台車。\n",
      "\n",
      "I might say yes.\n",
      "我可能会说是。\n",
      "\n",
      "I must help her.\n",
      "我必須幫助她。\n",
      "\n",
      "I need a friend.\n",
      "我需要个朋友。\n",
      "\n",
      "I need evidence.\n",
      "我需要證據。\n",
      "\n",
      "I need you here.\n",
      "我需要你在這裡。\n",
      "\n",
      "I paid the bill.\n",
      "我买了单。\n",
      "\n",
      "I played tennis.\n",
      "我打網球了。\n",
      "\n",
      "I run every day.\n",
      "我每天跑步。\n",
      "\n",
      "I speak Swedish.\n",
      "我说瑞典语。\n",
      "\n",
      "I talked to her.\n",
      "我跟她谈了话。\n",
      "\n",
      "I teach Chinese.\n",
      "我教中文。\n",
      "\n",
      "I think it's OK.\n",
      "我想沒關係。\n",
      "\n",
      "I took a shower.\n",
      "我洗了澡。\n",
      "\n",
      "I want a guitar.\n",
      "我想要一把吉他。\n",
      "\n",
      "I want that bag.\n",
      "我想要那個袋子。\n",
      "\n",
      "I want to drive.\n",
      "我想開車。\n",
      "\n",
      "I was surprised.\n",
      "我吃惊了。\n",
      "\n",
      "I wish you'd go.\n",
      "我希望你去。\n",
      "\n",
      "I woke up early.\n",
      "我起得早。\n",
      "\n",
      "I work too much.\n",
      "我工作得太多了。\n",
      "\n",
      "I'll bring wine.\n",
      "我会带酒来。\n",
      "\n",
      "I'll never stop.\n",
      "我絕不會停。\n",
      "\n",
      "I'm a foreigner.\n",
      "我是一個外國人。\n",
      "\n",
      "I'm a night owl.\n",
      "我是個夜貓子。\n",
      "\n",
      "I'm about ready.\n",
      "我快好了。\n",
      "\n",
      "I'm always here.\n",
      "我一直在這裡。\n",
      "\n",
      "I'm daydreaming.\n",
      "我在做白日梦。\n",
      "\n",
      "I'm feeling fit.\n",
      "我覺得精神很好。\n",
      "\n",
      "I'm left-handed.\n",
      "我是左撇子。\n",
      "\n",
      "I'm not serious.\n",
      "我不是认真的。\n",
      "\n",
      "I'm out of time.\n",
      "我没时间了。\n",
      "\n",
      "I'm really busy.\n",
      "我真的好忙。\n",
      "\n",
      "I'm really cold.\n",
      "我真的冷。\n",
      "\n",
      "I'm still angry.\n",
      "我还饿着呢。\n",
      "\n",
      "I'm very hungry.\n",
      "我很餓。\n",
      "\n",
      "I'm very lonely.\n",
      "我很寂寞。\n",
      "\n",
      "I've had enough.\n",
      "我已經吃飽了。\n",
      "\n",
      "I've had enough.\n",
      "我已經吃飽了。\n",
      "\n",
      "Is Tom Canadian?\n",
      "Tom是加拿大人嗎?\n",
      "\n",
      "Is he breathing?\n",
      "他在呼吸嗎?\n",
      "\n",
      "Is it all there?\n",
      "全都在那裡嗎？\n",
      "\n",
      "Is it too salty?\n",
      "还有多余的盐吗？\n",
      "\n",
      "Is she Japanese?\n",
      "她是日本人嗎？\n",
      "\n",
      "Is this a river?\n",
      "這是一條河嗎?\n",
      "\n",
      "Isn't that mine?\n",
      "那是我的吗？\n",
      "\n",
      "It is up to you.\n",
      "由你來決定。\n",
      "\n",
      "It snowed a lot.\n",
      "下了很多的雪。\n",
      "\n",
      "It was terrible.\n",
      "真糟糕。\n",
      "\n",
      "It was very far.\n",
      "它很遠。\n",
      "\n",
      "It'll be cloudy.\n",
      "天要变多云了。\n",
      "\n",
      "It's a dead end.\n",
      "这是个死胡同。\n",
      "\n",
      "It's a new book.\n",
      "那本書是一本新書。\n",
      "\n",
      "It's a nice day.\n",
      "今天天氣很好。\n",
      "\n",
      "It's a surprise.\n",
      "这是一个惊喜。\n",
      "\n",
      "It's almost six.\n",
      "这是免费的。\n",
      "\n",
      "It's already 11.\n",
      "已经是11点了。\n",
      "\n",
      "It's fine today.\n",
      "今天天气很好。\n",
      "\n",
      "It's impossible.\n",
      "這是不可能的。\n",
      "\n",
      "It's lunch time.\n",
      "午餐時間到了。\n",
      "\n",
      "It's okay to go.\n",
      "你可以走了。\n",
      "\n",
      "It's over there.\n",
      "在那里。\n",
      "\n",
      "It's time to go.\n",
      "是該離開的時候了。\n",
      "\n",
      "It's time to go.\n",
      "是該離開的時候了。\n",
      "\n",
      "Jesus loves you.\n",
      "耶穌愛你。\n",
      "\n",
      "Keep on smiling.\n",
      "保持微笑。\n",
      "\n",
      "Keep on working.\n",
      "繼續工作！\n",
      "\n",
      "Keep the change!\n",
      "不用找零钱了。\n",
      "\n",
      "Large, isn't it?\n",
      "很大, 不是嗎?\n",
      "\n",
      "Lemons are sour.\n",
      "檸檬是酸的。\n",
      "\n",
      "Let me go alone.\n",
      "讓我一個人去。\n",
      "\n",
      "Let me see that.\n",
      "讓我看看。\n",
      "\n",
      "Let them decide.\n",
      "讓他們決定。\n",
      "\n",
      "Let's eat sushi.\n",
      "讓我們吃壽司吧。\n",
      "\n",
      "Let's go by bus.\n",
      "讓我們坐公共汽車去。\n",
      "\n",
      "Let's not argue.\n",
      "我們別吵了。\n",
      "\n",
      "Let's turn back.\n",
      "我们掉头吧！\n",
      "\n",
      "Look at the sky.\n",
      "看天上。\n",
      "\n",
      "Look behind you.\n",
      "瞧你身後。\n",
      "\n",
      "Make it smaller.\n",
      "把它弄小一點。\n",
      "\n",
      "May I leave now?\n",
      "我能開車送到了吗？\n",
      "\n",
      "May I try it on?\n",
      "我能试一下吗？\n",
      "\n",
      "Maybe next time.\n",
      "也许下一次吧。\n",
      "\n",
      "Men should work.\n",
      "男人应该工作。\n",
      "\n",
      "Merry Christmas!\n",
      "聖誕快樂。\n",
      "\n",
      "Mom, I'm hungry.\n",
      "妈妈，我肚子饿了。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000,1100):\n",
    "    test = encoder_input[i:i+1,:,:]#i:i+1保持数组是三维\n",
    "    out = predict_chinese(test,encoder_infer,decoder_infer,OUTPUT_LENGTH,OUTPUT_FEATURE_LENGTH)\n",
    "    print(input_texts[i])\n",
    "    print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
